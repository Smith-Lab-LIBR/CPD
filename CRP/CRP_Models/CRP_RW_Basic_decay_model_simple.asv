% get probabilities of true action for each trial 

% get probabilities of true action for each trial 

function action_probs = CRP_RW_Basic_decay_model_simple(params, trials, test)
% parameters assignment
choices = [];
% learning_rate = 0.3;
% inverse_temp = 0.5;
% initial_value = 0;
% reward_prior_1 = 0;
% reward_prior_2 = 0;
% reward_prior_3 = 0;   

learning_rate = params.reward_lr;
inverse_temp = params.inverse_temp;
reward_prior = params.reward_prior;
alpha = params.alpha;
% loop over each trial 
choice_rewards = [0, 0, 0];
latent_states_distribution = [1];
latent_state_rewards = [reward_prior, reward_prior, reward_prior];
latent_state_counts = [1];
t_latent_state_counts = 1;
% retrieve true action/s and results 
for trial = 1:length(trials)
    current_trial = trials{trial};
    if height(current_trial) > 2
        true_actions = current_trial(2:3,2);
    else
        true_actions = current_trial(2,2);
    end
    trial_length = height(true_actions);
    results = current_trial(1,3);
    for t = 1:min(trial_length, 3)
        true_action = true_actions(t,1).response;
        if ~isempty(results)
            result = results.result; % correct choice 
        end
        reward_probabilities = softmax_rows(latent_state_rewards*inverse_temp);
        latent_states_distribution = latent_state_counts/t_latent_state_counts;
        if t == 1
             outcome = zeros(1,3);
            
    
            %% simulate action %%

            % sample choice from latent states
            action_probabilities = sum(latent_states_distribution' .* reward_probabilities, 1); % action probabilities are taken as an expectation over all latent states; but actual reward updates is with respect to a specific sampled latent state
            %[m, idx] = max(latent_states_distribution);
            %action_probabilities = reward_probabilities(idx,:);
            action_probs{trial}(t,:) = action_probabilities;
            u = rand(1,1);
            choice = find(cumsum(action_probabilities) >= u, 1);
            %[m, choice] = max(action_probabilities);
            choice = choice - 1; % match the coding of choices from task
            choices{trial}(t,:) = choice;
            
             % construct prior based on CRP
            new_latent_states_distribution = latent_state_counts/(t_latent_state_counts + alpha);
            new_latent_states_distribution(end+1) = alpha/(t_latent_state_counts + alpha);
            latent_state_rewards(end+1,:) = [reward_prior,reward_prior,reward_prior];
            if trial_length > 1    

                % we dont know what the correct answer is yet, so
                % likelihood can be computed with how wrong the model was
               % latent_state_likelihoods = latent_state_rewards;
                likelihood = softmax_rows(latent_state_rewards);
                update = 1 - likelihood(:, true_action + 1);
                latent_state_likelihoods = update;
                % calculate posterior
                post = new_latent_states_distribution.*latent_state_likelihoods';
                post = (post+exp(-16))/(sum((post+exp(-16))));
                u = rand(1,1);
                latent_state_choice = find(cumsum(post) >= u, 1); % the model deciding on a latent state, and then updating it's reward function for that latent state ; 
                prediction_error = learning_rate * (-1 - (latent_state_rewards(latent_state_choice,true_action + 1)));
                latent_state_rewards(latent_state_choice,true_action + 1) = latent_state_rewards(latent_state_choice,true_action + 1) + prediction_error;   
                if latent_state_choice ~= length(new_latent_states_distribution)
                    %latent_states_distribution(end) = [];
                    latent_state_rewards(end,:) = [];
                else
                    latent_states_distribution = new_latent_states_distribution;
                    latent_state_counts(end+1) = 0;
                end

                latent_state_counts = latent_state_counts * gamma;
                latent_state_counts(latent_state_choice) = latent_state_counts(latent_state_choice) + 1;
                t_latent_state_counts = t_latent_state_counts + 1;
                
            else
                % got it right on the first try, so the correct result is
                % just the true action
                latent_state_likelihoods = softmax_rows(latent_state_rewards);
                % calculate posterior
                post = new_latent_states_distribution.*latent_state_likelihoods(:,true_action+1)';
                post = (post+exp(-16))/(sum((post+exp(-16))));
                u = rand(1,1);
                latent_state_choice = find(cumsum(post) >= u, 1);
                outcome = outcome -1;
                outcome(true_action + 1) = 1;
                
                prediction_error = learning_rate * (outcome - latent_state_rewards(latent_state_choice,:));
                latent_state_rewards(latent_state_choice,:) = latent_state_rewards(latent_state_choice,:) +  prediction_error;
                if latent_state_choice ~= length(new_latent_states_distribution)
                    %latent_states_distribution(end) = [];
                    latent_state_rewards(end,:) = [];
                else
                    latent_states_distribution = new_latent_states_distribution;
                    latent_state_counts(end+1) = 0;
                    
                end

                latent_state_counts = latent_state_counts * gamma;
                latent_state_counts(latent_state_choice) = latent_state_counts(latent_state_choice) + 1;
                t_latent_state_counts = t_latent_state_counts + 1;
            end

        else
            if ~isempty(result)
                 % construct prior based on CRP

                new_latent_states_distribution = latent_state_counts/(t_latent_state_counts + alpha);
                new_latent_states_distribution(end+1) = alpha/(t_latent_state_counts  + alpha);
                latent_state_rewards(end+1,:) = [reward_prior,reward_prior,reward_prior];

                previous_result_idx = true_actions(t-1,1).response + 1;
                outcome = zeros(1,3);   
                outcome = outcome - 1;
                outcome(result + 1) = 1; % result - correct choice 
                reward_probabilities = softmax_rows(latent_state_rewards*inverse_temp);
                reward_probabilities(:,previous_result_idx) = exp(-16);
                row_sums = sum(reward_probabilities, 2); % Sum along the second dimension (rows)
                reward_probabilities = bsxfun(@rdivide, reward_probabilities, row_sums);

                action_probabilities = sum(new_latent_states_distribution' .* reward_probabilities, 1);
                action_probs{trial}(t,:) = action_probabilities;
                u = rand(1,1);
                choice = find(cumsum(action_probabilities) >= u, 1);
                %[m, choice] = max(action_probabilities);
                choice = choice - 1; % match the coding of choices from task
                choices{trial}(t,:) = choice;
                % update latent_state_distribution
                
                columnIndices = true(1, 3);
                columnIndices(previous_result_idx) = false;
                %outcome(sub_block_result.response + 1) = 1;
               likelihoods = reward_probabilities;
               latent_state_likelihoods = likelihoods(:,result+1);
                % calculate posterior
                post = new_latent_states_distribution.*latent_state_likelihoods';
                post = (post+exp(-16))/(sum((post+exp(-16))));
                u = rand(1,1);
                latent_state_choice = find(cumsum(post) >= u, 1);
                prediction_error = learning_rate * (outcome(:,columnIndices) - latent_state_rewards(latent_state_choice,columnIndices));
                
                % update latent state reward predictions weighted by latent state distribution
                latent_state_rewards(latent_state_choice,columnIndices) = latent_state_rewards(latent_state_choice,columnIndices) + prediction_error;
                % if we didnt sample a new latent state, remove it
                if latent_state_choice ~= length(new_latent_states_distribution)
                    %latent_states_distribution(end) = [];
                    latent_state_rewards(end,:) = [];
                else
                    latent_states_distribution = new_latent_states_distribution;
                    latent_state_counts(end+1) = 0;
                end

                latent_state_counts = latent_state_counts * gamma;
                latent_state_counts(latent_state_choice) = latent_state_counts(latent_state_choice) + 1;
                t_latent_state_counts = t_latent_state_counts + 1;
            end
        end
        
    end
end

end



%% functions 


function [c,isnew,pmf] = rand_ddCRP(alpha,A,slope,baserate,c_old,N_causes,t)

if isempty(c_old)
    c = 1; isnew = 1; pmf = 1;
    return;
end

% current time point
t_current = t(end);
% previous time points
t_old = t(1:end-1);

% probability for each cause
pmf = zeros(1,N_causes+1);
for i_cause = 1:N_causes
    deltat = t_current - t_old(c_old==i_cause);
    pmf(i_cause) = A*sum(exp(-slope*deltat)) + baserate;
end
pmf(end) = alpha; % new cause
pmf = pmf/sum(pmf); % normalization

% sample
cmf = cumsum(pmf);
c = find(rand()<=cmf,1,'first');

if c > N_causes
    isnew = 1;
else
    isnew = 0;
end

end

function [c,isnew,pmf] = CRP_basic(alpha,c_old,N_causes,t)

if isempty(c_old)
    c = 1; isnew = 1; pmf = 1;
    return;
end

% current time point
t_current = t(end);
% previous time points
t_old = t(1:end-1);

% probability for each cause
pmf = zeros(1,N_causes+1);
for i_cause = 1:N_causes
    deltat = t_current - t_old(c_old==i_cause);
    pmf(i_cause) = A*sum(exp(-slope*deltat)) + baserate;
end
pmf(end) = alpha; % new cause
pmf = pmf/sum(pmf); % normalization

% sample
cmf = cumsum(pmf);
c = find(rand()<=cmf,1,'first');

if c > N_causes
    isnew = 1;
else
    isnew = 0;
end

end

function matrix = softmax_rows(matrix)
    % Subtract the maximum value from each row for numerical stability
    matrix = bsxfun(@minus, matrix, max(matrix, [], 2));
    
    % Calculate the exponent of each element
    exponents = exp(matrix);
    
    % Calculate the sum of exponents for each row
    row_sums = sum(exponents, 2);
    
    % Divide each element by the sum of its row

    
    matrix = bsxfun(@rdivide, exponents, row_sums);
end
